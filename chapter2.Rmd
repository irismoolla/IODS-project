

## Regression and model validation


This week I learned new techniques of data wrangling and visualizing data before the data analysis itself. I liked especially the function ggpairs from the ggplot2 package. With the data that I prepared for the analysis, I performed and interpreted regression analysis with multiple variables. All the R codes, interpretations and explanations of the analysis exercises are found below. 


data source: [http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt](http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt) 


```{r}
date()
```


### Step 1

Reading the learning2014 data (named as lrndata) into from my local folder. By exploring the structure and the dimensions of the data, we can see that it has 166 rows, in other words 166 observations, and 7 columns, in other words 7 variables. Data consists of variables that contain information about students' age (in years) (variable age), attitudes toward statistics (variable attitude), exam points (variable points), gender (variable gender), and their learning approaches: deep learning (variable deep), strategic learning (variable stra) and surface learning (variable surf). 


```{r}
setwd("~/Git/IODS-project")
lrndata <- read.table ("C:/Users/irisp/Documents/Git/IODS-project/data/lrndata.csv")
str(lrndata)
head(lrndata)

```

### Step 2 

Below is a graphical overview of the data which shows (1) scatterplots of each variable pair visualized on left side, (2) variable distributions on the diagonal and (3) Pearson correlation value and significance displayed on the right side of the plot. The colours of the overview represent the values of the gender variable (females and males).

The distributions of the variables tell us that there are more females than males in the dataset and the age distribution is skewed towards the left which means there are more younger students than older students.Also, males give higher values for the attitudes toward statistics than females, otherwise the variable distributions resemble each other for males and females and are relatively standard. 

The attitude toward statistics correlates significantly positively with the students' exam points and significantly negatively with males' surface learning orientation. Deep learning correlates significantly negatively with males' surface learning approach and strategic learning correlates significantly negatively with surface learning approach. Other variables do not correlate with each other significantly and the correlations between them are relatively weak. 

Scatterplots display the relationship between two variables and each dot represents an observation. The variables that correlate with each other significantly, it is possible to detect some patterns of observations such as in the scatterplot between variables points and attitude. It is also possible to detect some outlier observations such as in the scatterplot between variables deep and surf.  


```{r}
library(GGally)
library(ggplot2)

plot1 <- ggpairs(lrndata, mapping = aes(col = gender, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
plot1
```


Below you can see summaries of variables in the data. For the numeric variables it shows the minimum, maximum, median, mean and cutpoint values for first quartile (25%) and third Quartile (75%). Further inspecting gender variable, we can see there are 110 females and 56 males in the data.  


```{r}
summary(lrndata)
table (lrndata$gender)
```


### Step 3

For the regression model, I use students' attitudes toward statistics, strategic learning approach and gender to explain students' exam points. Below you can the summaries of two fitted models; model 1 with all three explanatory variables and model 2 only with the explanatory variable (attitude toward statistics) that has a statistically significant relationship with the target variable. The other two explanatory variables,  strategic learning approach and gender, did not have a statistically significant relationship with exam points, and they were removed from the model and the model 2 was fitted again without them. 

The regression model 1 consists of a set of three explanatory variables. The model output gives a summary of the model residuals which correspond to the prediction errors of the model. The coefficient part shows the estimates of the parameters in the model. The estimates tell us that the effect of attitudes on exam points is 0.35 with a standard error of 0.06. The parameter estimate tells us that a one unit-unit change in the attitude variable produces a 0.35-unit increase in exam points when all the other explanatory variables is being held constant. The t value corresponds to a statistical test of a null hypothesis so it tests the significance of the coefficient. T value is obtained by dividing the regression parameter estimate by its standard error. P value for the attitude variable is highly significant so we can conclude that there is a statistical relationship between exam points and attitudes toward statistics. 

Same interpretation goes with the stra variable: its parameter estimate tells us that a one unit-unit change in the stra variable produces a 0.89-unit increase in exam points when all the other explanatory variables is being held constant. The P value for the stra variable is not significant so we cannot say anything certain about the statistical relationship between exam points and strategic learning approach.

The coefficient of gender indicates that males have -0.22 lower exam points than females when other explanatory variables are controlled. Also, the P value for the gender variable is not significant so there is no evidence of the statistical relationship between exam points and gender. 


```{r}
# fit a linear model
model1 <- lm(points ~ attitude + stra + gender, data = lrndata)

# print out a summary of the model
summary(model1)

# fit a linear model
model2 <- lm(points ~ attitude, data = lrndata)

# print out a summary of the model
summary(model2)

```


### Step 4

Using a summary of your fitted model, explain the relationship between the chosen explanatory variables and the target variable (interpret the model parameters). Explain and interpret the multiple R squared of the model. (0-3 points)

The summary of the model 2 tell us that the effect of attitudes on exam points is 0.35 with a standard error of 0.06 and we can see there is no change in the parameters between models 1 and 2. Again, a one unit-unit change in the attitude variable produces a 0.35-unit increase in exam points. P value for the attitude variable is highly significant, p < 0.000. 

The multiple R squared of the model tells us how well the model fits the data. It tells us that variation in attitudes explains 19.1% of the variance in exam points (multiple R-squared gets a value 0.1906). With only one explanatory variable, the multiple R-squared is the square of the correlation coefficient between exam points and attitudes. 


### Step 5 

I produced the following diagnostic plots for the model 2: Residuals vs Fitted values, Normal QQ-plot and Residuals vs Leverage. With these plots it is possible to check the assumptions of linear regression models: (1) the errors are normally distributed, (2) the errors are not correlated, (3) the errors have constant variance, and (4) the size of a given error does not depend on the explanatory variables.

To check the constant variance of errors, we look at the residuals vs Fitted values plot, which is scatter plot of residuals and  model predictions. There are no patterns in the scatter plot so we can say that the model errors have constant variance.
 
To check the normality of the errors we look at the QQ-plot of the residuals. The residuals seem to fall onto the line quite well which means that the errors of the model fit to the normality assumption. 

To check that the size of a given error is not dependent on the explanatory variables, we look at the residuals vs leverage plot which measures the impact of a single observation on the model. We can see that any of the observations is not specifically pulling the regression line upwards or downwards. We can say that none of the observations has an unusually high impact on the size of the given errors. 


```{r}
par(mfrow = c(2,2))
plot(model2, which = c(1,2,5))
```


