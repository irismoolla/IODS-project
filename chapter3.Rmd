
## Logistic Regression


This week I learned new techniques of data wrangling and visualizing data before the data analysis itself. I liked especially the function ggpairs from the ggplot2 package. With the data that I prepared for the analysis, I performed and interpreted logisctic regression analysis. All the R codes, interpretations and explanations of the analysis exercises are found below. 


data source: [https://archive.ics.uci.edu/ml/datasets/Student+Performance](https://archive.ics.uci.edu/ml/datasets/Student+Performance) 


```{r}
date()
```


### Steps 1 & 2

I created a new R Markdown file named 'chapter3.Rmd' to perform this week's analyses in it. I read the joined student alcohol consumption data into R from your local folder and printed out the names of the variables in the data.

This data includes information on student achievement in secondary education of two Portuguese schools. The data attributes include student grades (G1 = average first period grade of maths and Portuguese, G2 = average second period grade of maths and Portuguese, G3 = average final grade of maths and Portuguese, G1-3.p = Portuguese grades, G1-3.m = maths grades), demographic (e.g. sex = gender, age, famsize = family size, Medu = mother's education, Fedu = father's education), social and school related features (e.g. romantic = with a romantic relationship, absences = average number of school absences, alc_use = weekly alcohol use, high_use = high level of weekly alcohol use), and it was collected by using school reports and questionnaires. The dataset combines two datasets that provide data regarding the performance in mathematics and Portuguese language. More information about the variables in the dataset can be found here: [https://archive.ics.uci.edu/ml/datasets/Student+Performance](https://archive.ics.uci.edu/ml/datasets/Student+Performance). 

```{r}
setwd("~/Git/IODS-project")
pormath <- read.table ("C:/Users/irisp/Documents/Git/IODS-project/data/pormath.csv")
colnames(pormath)
```

### Step 3
The purpose of this analysis is to study the relationships between high/low alcohol consumption and the following 4 variables in the data: quality of family relationships (famrel), average number of school absences (absences), gender (sex) and going out with friends (goout).

My personal hypotheses about the relationships of those variables with alcohol consumption:
1. Males have higher probability of consuming more alcohol than females. 
2. Higher number of average of school absences increases the probability of consuming more alcohol.
3. Better quality family relationships decrease the probability of consuming more alcohol.
4. The more students go out with their friends, the higher probability they have of consuming more alcohol.


### Step 4

First, we graphically explore the distributions of my chosen variables. Below you can see histograms which show the distributions of the variables. We can see that the school absences are concentrated on the two ends, low and high absences. Alcohol consumption is skewed towards left which means there are more students who use less alcohol which can also been seen from the distribution of the high alcohol use variable. Also quality of family relationships is skewed, which means there are more students whose family relationships  are good than bad. Going out with friends variable has a normal distribution, and there are a bit more female than male students in the data. 

```{r}
# access the tidyverse libraries tidyr, dplyr, ggplot2
library(tidyr); library(dplyr); library(ggplot2); library(GGally)

# A bar plot of each variable
gather(pormath[c(2,22,24,33,49,50)]) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar()

```


Below is a graphical overview of the data which shows (1) boxplots and  scatterplots of each variable pair visualized on left and upper side, (2) variable distributions on the diagonal and (3) Pearson correlation value and significance displayed on the right side of the plot. The colours of the overview represent the values of the gender variable (females and males).

The box plots show the gender wise distribution of the chosen variables and their lower and higher quartiles and median. Red colour represents female gender and blue colour male gender. The boxplot also show some dots that are outliers or extreme values. 

The distributions of the variables show us the same distributions as the histograms above but now we can see the distributions separately for males and females. 

The absences and going out variables correlate significantly positively with the students’ alcohol use. Alcohol use correlates significantly negatively with males’ quality of family relationship. Based on these explorations, we can compare the results to my previously stated hypotheses. It seems that all of my hypotheses find support from these results: male consume more alcohol, higher number of average of school absences increases alcohol consumption, better quality family relationships decrease alcohol consumption of males and the more students go out with their friends, the more they consume alcohol. 


```{r}

plot1 <- ggpairs(pormath, mapping = aes(col = sex, alpha = 0.3), 
                 columns = c(2,22,24,33,49), lower = list(combo = wrap("box")))
plot1

```



Similar patterns and relationships can be also detected from the cross tabulation tables below. 


```{r}

# Cross tabulation: summary statistics by group
pormath %>% group_by(sex, high_use) %>% summarise(count = n())

pormath %>% group_by(absences, high_use) %>% summarise(count = n())

pormath %>% group_by(famrel, high_use, sex) %>% summarise(count = n())

pormath %>% group_by(goout, high_use) %>% summarise(count = n())

```


### Step 5

Second, I use logistic regression to statistically explore the relationship between my chosen variables and the binary high/low alcohol consumption variable as the target variable. From the summary of the model m, we can see that all the explanatory variables are highly significant and so we can conclude that there is a statistical relationship between high alcohol use and all the explanatory variables. We can also see from the logistic regression coefficients that male students are more likely to have high alcohol consumption than females, having more absences at school and going out with friends more increase high alcohol consumption and having good quality family relationships decrease high alcohol consumption of students. 


```{r}

m <- glm(high_use ~ sex + absences + famrel + goout, data = pormath, family = "binomial")

# print out a summary of the model
summary(m)

```
Odds ratios express probabilities and higher odds ratio means a higher probability of something to happen. The sex variables has a odds ratio of 2.94 which means that males have 2.94 times the odds of have high alcohol consumption than females. For students who go out with their friends more, we can say that each increase in the variable going out increases the odds of having high alcohol consumption by 8%.

When odds ratio is less than 1.00, we need to substract the odds ratio from 1.00 to find the decrease in odds ratio. Thus, better quality family relationships decrease the odds of having high alcohol consumption by (1.00-0.65)x100 = 35%.  

Also the odds ratios support the hypotheses that I made earlier. We can say that based of the results of the logistic regression: 1. Males have higher probability of consuming more alcohol than females, 2. Higher number of average of school absences increases the probability of consuming more alcohol, 3. Better quality family relationships decrease the probability of consuming more alcohol, and 4. The more students go out with their friends, the higher probability they have of consuming more alcohol.


```{r}

# compute odds ratios (OR)
OR <- coef(m) %>% exp

# compute confidence intervals (CI)
CI <- confint(m) %>% exp

# print out the odds ratios with their confidence intervals
cbind(OR, CI)

```


### Step 6

Using all the variables which, according to my logistic regression model, had a statistical relationship with high/low alcohol consumption, I explore the predictive power of my model. Below you can see a 2x2 cross tabulation of predictions versus the actual values. It seems that the model has correctly predicted 243 as low and 53 as high alcohol users and wrongly predicted 58 cases as low alcohol users and 16 cases as high alcohol users. 

```{r}

# predict() the probability of high_use
probabilities <- predict(m, type = "response")

# add the predicted probabilities to the data
pormath <- mutate(pormath, probability = probabilities)

# use the probabilities to make a prediction of high_use
pormath <- mutate(pormath, prediction = pormath$probability > 0.5)

# tabulate the target variable versus the predictions
table(high_use = pormath$high_use, prediction = pormath$probability > 0.5)

```

Below is a graphic visualizing both the actual values and the predictions.

```{r}
# initialize a plot of 'high_use' versus 'probability' 
g <- ggplot(pormath, aes(x = probability, y = high_use, col = prediction))
g + geom_point()
```

We can measure the performance in binary classification with accuracy which represents the average number of correctly classified observations. I computed the total proportion of inaccurately classified individuals (= the training error) which is 0.3 and means that the model has wrongly predicted 30% of the values, i.e. correctly predicted 70% of the values. The model predictions are better than random guessing or chance since the accuracy of the binary classification is more than 50%. 


```{r}
# define a loss function (mean prediction error)
loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}

# call loss_func to compute the average number of wrong predictions in the data
#Execute the call to the loss function with prob = 0, meaning you define the probability of high_use as zero for each individual
loss_func(class = pormath$high_use, prob = 0)

```

### Step 7

I performed 10-fold cross-validation on the model m. My model seems to have better test set performance (smaller prediction error using 10-fold cross-validation) compared to the model introduced in DataCamp (which had about 0.26 error). My model has 0.21 prediction error which is smaller than the DataCamp training model. Cross-validation is a method of testing a predictive model on unseen data and it can also be used to compare different models. The lower the value is, the better the model performance and the actual predictive power of the model. 


```{r}
# K-fold cross-validation
library(boot)

cv <- cv.glm(data = pormath, cost = loss_func, glmfit = m, K = 10)

# average number of wrong predictions in the cross validation
cv$delta[1]
```

