
## Dimensionality reduction techniques

This week I learned new techniques to reduce the dimensionality of a dataset. With the data already saved into R, I performed and interpreted principal component analysis and Multiple Correspondence Analysis. All the R codes, interpretations and explanations of the analysis exercises are found below. 

data source: [http://hdr.undp.org/en/content/human-development-index-hdi](http://hdr.undp.org/en/content/human-development-index-hdi) 


```{r}
date()
```


### Step 1: 

The data used in this exercise is a joined data set of Human Development Index and Gender Development Index that is available on the United Nations Development Programme’s website (link above). The human data set consists of 155 countries (observations) and 8 variables.

Description of the variables in the human data:

gni  = Gross National Income (GNI) per capita
lifex = Life expectancy at birth
expedu = Expected years of schooling 
matmor = Maternal mortality ratio
adlbr = Adolescent birth rate
reppar = Percentage of female representatives in parliament
ratio_2edu = the ratio of Female and Male populations with secondary education in each country (edu2F / edu2M)
ratio_lab = the ratio of labour force participation of females and males in each country (labF / labM)

The dimensions of the data and summaries of the variables are shown below.


```{r}

setwd("~/Git/IODS-project")
human <- read.table("C:/Users/irisp/Documents/Git/IODS-project/data/human.csv")

# access the following packages
library(tidyr); library(dplyr); library(ggplot2); library(GGally); library(corrplot)

dim (human)
summary (human) 

```

Below you can see two graphical overviews of the data. From the ggpairs visualization we can see that variables ratio_edu2 and expedu have a normal distribution, the distributions of the variables ratio_lab and lifex are skewed towards right (meaning there are more higher values than lower values) and the distributions of the rest of the variables is skewed towards left (meaning there are more lower than higher values). 


```{r}
# visualize the 'human' variables
ggpairs(human)
```
From the ggpairs plot and correlation plot (corrplot) we can comment on the relationships between the variables. 

We can see positive and statistically significant correlations between the following variables:

* lifex and ratio_2edu, gni and reppar, meaning that the higher the life expectancy in a country is, the higher the ratio of female and male populations with secondary education, GNI and percentage of female representatives in parliament are.
* expedu and ratio_2edu, lifex, gni and reppar, meaning that the higher expected years of schooling is, the higher the ratio of female and male populations with secondary education is, life expectancy, GNI and percentage of female representatives in parliament are. 
* gni and ratio_2edu, lifex and expedu, meaning that the higher the GNI is, the higher the ratio of female and male populations with secondary education is, life expectancy and expected years of schooling are. 
* matmor and adlbr and ratio_lab, meaning that  the higher the maternal mortality ratio is, the higher the adolescent birth rate and the ratio of labour force participation of females and males are. 
* reppar and ratio_lab, lifex and expedu, meaning that the the higher the percentage of female representatives in parliament are,  the higher the ratio of labour force participation of females and males, life expectancy and expected years of schooling are. 

We can see negative and statistically significant correlations between the following variables:

* lifex and matmor and adlbr, meaning that  the higher the life expectancy in a country is, the lower the maternal mortality ratio and adolescent birth rate are. 
* expedu and matmor and adlbr, meaning that  the higher the expected years of schooling in a country is, the lower the maternal mortality ratio and adolescent birth rate are. 
* gni and matmor and adlbr, meaning that  the higher the GNI of a country is, the lower the maternal mortality ratio and adolescent birth rate are. 
* matmor and ratio_2edu, lifex, expedu and gni, meaning that the higher the maternal mortality ratio is, the lower the the ratio of female and male populations with secondary education, life expectancy, expected years of schooling and GNI are. 
* adlbr and ratio_2edu, lifex, expedu and gni, meaning that the higher the adolescent birth rate is, the lower the the ratio of female and male populations with secondary education, life expectancy, expected years of schooling and GNI are. 


```{r}
# compute the correlation matrix and visualize it with corrplot
cor(human) %>% corrplot
```

### Step 2:


Next, Principal Component Analysis (PCA) is performed by the Singular Value Decomposition (SVD). The prcomp() function in R uses the SVD and is the preferred, more numerically accurate method. It decomposes a data matrix into a product of smaller matrices, which let's us extract the underlying principal components. This makes it possible to approximate a lower dimensional representation of the data by choosing only a few principal components.

First, the PCA is performed with the not standardized human data. From the summary of the PCA1, we can see that the data is transformed to 8 different dimensions called principal components. Most variability in the data are captured by the first and second principal components (see standard deviations). The 1st principal component captures the maximum amount of variance from the features in the original data; the 2nd principal component captures the maximum amount of variability left and this is true for each of the following principal components. All the principal components are all uncorreleated and each of them is less important than the previous one, in terms of captured variance.


```{r}

# perform principal component analysis (with the SVD method)
pca1_human <- prcomp(human)

pca1 <- summary(pca1_human)

pca1
```

Below is a biplot of the PCA with unstandardized data displaying the observations by the first two principal components (PC1 coordinate in x-axis, PC2 coordinate in y-axis), along with arrows representing the original variables. 

A biplot is a way of visualizing the connections between two representations of the same data. A biplot consists of a scatter plot in which the observations are placed on x and y coordinates defined by two principal components (PC's) and arrows, which visualize the connections between the original variables and the PC's. The following connections hold:

* The angle between the arrows can be interpret as the correlation between the variables.
* The angle between a variable and a PC axis can be interpret as the correlation between the two.
* The length of the arrows are proportional to the standard deviations of the variables

From the figure below, it is difficult to see the arrows and thus, interpret it properly. The first PC captures all the variation in the data and that is the reason why PC2 captures 0 % of the variability. GNI feature contributes to the first PC as its arrow is pointing towards the direction of the x-axis and even though, we cannot see the arrows of the other features, they probably contribute to the first PC as well because of the amount of the variance the first PC captures. 

```{r}


biplot(pca1_human, choices = 1:2, cex = c(0.8, 1), col = c("grey40", "deeppink2"))

# rounded percentages of variance captured by each PC
pca_pr <- round(100*pca1$importance[2,], digits = 1) 

# print out the percentages of variance
pca_pr

# create object pc_lab to be used as axis labels
pc_lab <- paste0(names(pca_pr), " (", pca_pr, "%)")

# draw a biplot of the principal component representation and the original variables
biplot(pca1_human, cex = c(0.6, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])

```


### Step 3:


Second, I standardize the variables in the human data and repeat the above principal component analysis. From the summary of the PCA2, we can see the first PC again captures most variability in the data and the second PC the second most. The results between PCA1 and PCA2 differ because of the standardization of the data. Standardization is needed to get more accurate results as PCA is sensitive to the relative scaling of the original features and assumes that features with larger variance are more important than features with smaller variance.


```{r}

# standardize the variables
human_std <- scale(human)

# perform principal component analysis (with the SVD method)
pca2_human <- prcomp(human_std)

pca2 <- summary(pca2_human)

pca2
```
From the figure below, we see that the first PC captures 50.6 % and  second PC 16.2 % of the variation in the data. Expedu, lifex, ratio_2edu and gni features contribute to the first PC as well as matmor and adlbr, even though they point to the opposite direction than the other four featues. When features diverge and form a large angle (close to 180°), they are negatively correlated with each other which the correlations and correlation plot above also confirm: expedu, lifex, ratio_2edu and gni correlate strongly with each other and all of them correlate negatively with  matmor and adlbr, that in turn, correlate quite highly positively with each other. We can see this from the vectors that each of the features have: when the vectors are close to each other and form a small angle, the variables they represent are positively correlated. 

Reppar and ratio_lab features in turn contribute to the second PC as their arrows point to to the direction of the y-axis. Reppar and ratio_lab meet the other features at 90°, which means those variables they represent are not likely to be correlated. This is also supported by the descriptive statistics at the beginning of this exercise.

Compared to biplot of PCA1, the observations (countries) are more spread accross the scatter plot; in the PCA1 biplot the observations were concentrated on the top right corner of the plot. This difference is also due to the different scaling of the data. Also, in the PCA1, GNI feature is assumed to be more important than the other features because GNI has the largest variance of the original scale of the data. The features with smaller variance are all concentrated on one point in the PCA1 biplot but when the original scales are standardized all the features are assumed to have the same importance and none of them stand out in the PCA2 biplot. 


```{r}

# rounded percentages of variance captured by each PC
pca_pr_std <- round(100*pca2$importance[2,], digits = 1) 

# print out the percentages of variance
pca_pr_std

# create object pc_lab to be used as axis labels
pc_lab_std <- paste0(names(pca_pr_std), " (", pca_pr_std, "%)")

# draw a biplot
biplot(pca2_human, cex = c(0.5, 0.8), col = c("grey40", "deeppink2"), 
       xlab = pc_lab_std[1], ylab = pc_lab_std[2])

```

### Step 4:

I would interpret that the first two principal component dimensions based on the biplot drawn after PCA on the standardized human data represent following: 

* The first principal component is influenced by these features: Gross National Income per capita, life expectancy at birth, expected years of schooling, the ratio of Female and Male populations with secondary education in each country, maternal mortality ratio and adolescent birth rate. I would think this PC represents the development of a society in terms of wealth, health and education. 
* The second principal component is influenced by these two features: percentage of female representatives in parliament and the ratio of labour force participation of females and males in each country. I would think this PC represents gender inequality in labour market.


### Step 5:

Lastly, I perform the Multiple Correspondence Analysis on the tea data (loaded from the package Factominer). Before the analysis, I explored the structure and the dimensions of the data and visualized it after selecting six columns of the data to keep for the analysis. 

```{r}
#install.packages("FactoMineR")
library(FactoMineR)

# load the data
data("tea")
str(tea)
dim(tea)

# column names to keep in the dataset
keep_columns <- c("Tea", "How", "how", "sugar", "where", "lunch")

# select the 'keep_columns' to create a new dataset
tea_time <- dplyr::select(tea, one_of(keep_columns))

# look at the summaries and structure of the data
summary(tea_time)
str(tea_time)

# visualize the dataset
gather(tea_time) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))


```

Next, Multiple Correspondence Analysis (MCA) is performed on the tea data.  MCA is a method to analyze qualitative adn/or categorical data and it is an extension of Correspondence analysis (CA). MCA can be used to detect patterns or structure in the data as well as in dimension reduction, just like PCA.

MCA summary contains the following information of the MCA dimensions: 

* Eigenvalues, which are the variances and the percentage of variances retained by each dimension and as in the PCA, the first dimension captures most variability in the data and the following less and less. 
* Individuals, which are the individuals coordinates, the individuals contribution (%) on the dimension and the cos2 (the squared correlations) on the dimensions.
* Categories, which are the coordinates of the variable categories, the contribution (%), the cos2 (the squared correlations) and v.test value. The v.test follows normal distribution: if the value is below/above ± 1.96, the coordinate is significantly different from zero. For the first dimension, only two categories are not significantly different from zero: sugar and alone, others are significant.  For the second dimension, only category black is not significant and for the third dimension, categories green, milk, tea bag, tea bag+unpackaged and unpackaged are not significant.   
* Categorical variables which represent the squared correlation between each variable and the dimensions. If the value is close to one, it indicates a strong link. The original categorical variables how and where influence strongly on the first dimension but also on the second dimension; and tea, How and sugar on the third dimension. Variable lunch does not share a strong link with none of the first three dimensions. 


```{r}

# multiple correspondence analysis
mca <- MCA(tea_time, graph = FALSE)

# summary of the model
summary(mca)

```
I have drawn below a variable biplot of the analysis where are variables drawn on the first two dimensions. The MCA biplot visualizes variable patterns in the data and the distances between variable categories gives a measure of their similarity. For example categories milk, Earl grey and sugar are more similar to each other than black and no sugar which in turn are similar. Also not lunch and alone are similar to each other as their distance from each other is short. Unpackaged and tea shop are different from all the other categories since they are located further away from the others. We can also see that first dimension captures 15.24 % of the variation and second dimension 14.23 % of the variation in the data. 

```{r}

# visualize MCA
plot(mca, invisible=c("ind"), habillage = "quali", graph.type = "classic")

```

