
## Clustering and Classification

This week I learned new techniques of data clustering and classification. With the data already saved into R, I performed and interpreted k-means clustering and linear discriminant analysis. All the R codes, interpretations and explanations of the analysis exercises are found below. 

data source: the Boston data from the MASS package 


### Step 1: 

Boston dataset contains 506 rows and 14 columns (506 observations, and 14 variables) and describes housing Values in Suburbs of Boston. Data consists of variables such as per capita crime rate by town (variable crim), average number of rooms per dwelling (variable rm), the proportion of blacks by town (variable black) and median value of owner-occupied homes in \$1000s (variable medv). 

More information about the variables in the Boston dataset can be found here: [https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html).

```{r}

library(MASS)

# load the data
data("Boston")
str(Boston)
dim(Boston)

```

### Step 2

First, we look at the summaries of the variables in the Boston data. We see that the data contains mostly numeric variables with the exception of rad which is an integer index variable and chas which is a binary variable. .  

```{r}
# access the following packages
library(tidyr); library(dplyr); library(ggplot2); library(GGally); library(corrplot)

summary(Boston)

```


Below we can see a graphical overview of histograms of all the variables in the Boston data, combined with the density curves. 

* age variable tells us the proportion of owner-occupied units built prior to 1940. This variable is slightly skewed towards right which mean that there is a high proportion of owner occupied units built prior to 1940. 
* Also variables black is skewed towards right which means that there is a high proportion of blacks in town. 
* Variables crim and zn are heavily skewed towards left which means that there is low proportion of per capita crime rate by town and residential land zoned for lots over 25,000 sq.ft. 
* Also the variables dis and lstat are skewed towards right which tells us that the mean of distances from the Boston suburbs to five Boston employment centres is relatively low, and a low proportion of the population in the Boston suburbs have lower status.
* Variable chas tells us that there are more suburbs that do not bound the Charles River than which do.
* Variables medv (median value of owner-occupied homes in \$1000s) and rm (average number of rooms per dwelling) are normally distributed. Also, variables tax (full-value property-tax rate per \$10,000) and rad (index of accessibility to radial highways) would have normal distribution but both of them have outlier variables at the right end of the distribution. 
* nox variable is slightly skewed towards left which means that there is a lower proportion of nitrogen oxides concentration in the Boston suburbs.
* ptratio variable seems to have a slightly skewed distribution towards right which tells us that there is a higher proportion of pupil-teacher ratio by town.
* indus variable has two curves in its distribution which means that the proportion of non-retail business acres per town is either low or high.


```{r}

# A bar plot of each variable
gather(Boston) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + 
  geom_histogram(aes(y = ..density..), 
                   colour="black", fill="white", bins = 18,) +
  geom_density(alpha = 0.2, fill = "#FF6666")

```
Below we can see a graphical overview of the correlations of the all the variables in the Boston data.

We can see high positive correlations between the following variables:

* medv and rm, meaning that the higher the median value of owner-occupied homes is, the higher the average number of rooms per dwelling is.
* lstat and age, nox and indus, meaning that the higher the lower the proportion of the lower status population is, the higher the age of the buildings, the nitrogen oxides concentration and the proportion of non-retail business acres per town are.
* tax and rad, nox, indus and crim, meaning that the higher the full-value property-tax rate is, the better the accessibility to radial highways is and the higher the nitrogen oxides concentration, the proportion of non-retail business acres per town and per capita crime rate are.
* rad and nox, indus and crim, meaning that the better the accessibility to radial highways is, the higher the nitrogen oxides concentration, proportion of non-retail business acres and per capita crime rate are.
* dis and zn, meaning that the longer the distance from the Boston employment centres is, the higher the proportion of residential land zoned for lots over 25,000 sq.ft is. 
* age and nox and indus, meaning that the older the age of the buildings, the higher the nitrogen oxides concentration and the proportion of non-retail business acres per town are. 
* nox and indus, meaning that the higher the nitrogen oxides concentration, the higher the proportion of non-retail business acres per town is.


We can see high negative correlations between the following variables:

* dis and age, nox and indus, meaning that the longer the distance from the Boston employment centres is, the lower the age of the buildings, the nitrogen oxides concentration and the proportion of non-retail business acres per town are.
* medv and lstat, meaning  that the higher the median value of owner-occupied homes is, the lower the proportion of the lower status population is. 
* lstat and rm, meaning that the higher the lower the proportion of the lower status population is, the lower the average number of rooms per dwelling is. 

```{r}

# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round(digits = 2)

# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex = 0.6)

```

### Step 3

Standardize the dataset and print out summaries of the scaled data. Now, we can see that compared to the non-standardized dataset, all the variables in the scaled dataset have been rescaled to have a mean of zero and a standard deviation of one.


```{r}
#standardizing Boston data 
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# class of the boston_scaled object
class(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
```

Next, I created a categorical variable of the crime rate in the Boston dataset (from the scaled crime rate) and used quantiles as the break points in the categorical variable. In addition, I dropoed the old crime rate variable from the dataset. 

```{r}

# summary of the scaled crime rate
summary(boston_scaled$crim)

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label=c("low", "med_low", "med_high", "high"))

# look at the table of the new factor crime
table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
```

Here, we divide the Boston dataset into train and test sets, so that 80% of the data belongs to the train set.

```{r}

# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

```

### Step 4

Below, I perform a Linear Discriminant analysis which is a classification (and dimension reduction) method. It finds the (linear) combination of the variables that separate the classes of the target variable which can be binary or multiclass variable.

```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

```

LDA can be visualized with a biplot which is done below: 

```{r}
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2.3)

```


### Step 5

Here, first, I save the crime categories from the test set and then remove the categorical crime variable from the test dataset. Second, I predict the classes with the LDA model on the test data and cross tabulate the results with the crime categories from the test set. 

We can see that the LDA model predicted the all the class high of the crime rate categories correctly. Med_high is also mostly correctly predicted, even though the LDA model 2 of the med_high classes were predicted as high and 3 as med_low. The model predicted med_low worse, as only half (15) of the classes were correctly predicted. For med_low, the other half was predicted as med_high (8) or low (4). The worst prediction was done for low, as only third (7) of the classes were correctly predicted. Mostly they were predicted as med_low (14) and one as med_high.

```{r}

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)

```


### Step 6

Here, the Boston dataset is reloaded and standardized. I have calculated the distances between the observations of the data with Euclidean distance which is the most common or "normal" distance measure. Similarity or dissimilarity of objects can be measured with distance measures. 

```{r}

# reload the data
data("Boston")

#standardizing Boston data 
boston_scaled <- scale(Boston)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

# euclidean distance matrix
dist_eu <- dist(Boston)

# look at the summary of the distances
summary(dist_eu)

```

Next, we run k-means algorithm on the dataset. K-means is the most known clustering method. It is an unsupervised method, that assigns observations to groups or clusters based on similarity of the objects.

```{r}
km <-kmeans(Boston, centers = 3)

```


K-means needs the number of clusters set in advance and the optimal number of clusters can be determined by looking at how the total of within cluster sum of squares (WCSS) behaves when the number of cluster changes. Plotting the number of clusters and the total WCSS, the optimal number of clusters is when the total WCSS drops radically.

```{r}
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

```

When investigating the optimal number of clusters, we see that there is a radical drop in the total WCSS when the number of clusters is 2. I run k-means algorithm again setting the number of clusters as 2. From the visualization of the clusters, we can see that the data clusters quite well between the two clusters, and the observation pairs seem to cluster around the two clusters (colours black and red represent the clusters) and the observations are separated from each other depending on the cluster they belong to. However, there are a few variables, such as rm and black, that do not seem to cluster that well according to the two clusters. The observation pairs between rm and black and other variables seem to overlap and there are outlier observations in both clusters (separate black and red dots). 


```{r}

# k-means clustering
km <-kmeans(Boston, centers = 2)

#setting the cluster variable as categorical
class(km$cluster)
km$cluster <- as.factor(km$cluster)

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster) 

#taking a closer look at the clusters in the data
pairs(Boston[1:10], col = km$cluster)
pairs(Boston[6:14], col = km$cluster)


```

